{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mnist을 이미지로 변환하고, 변환한 이미지를 numpy로 다시 변환하여 학습시킨 결과와 mnist의 학습결과를 비교해 보겠습니다. 다음과 같은 순서로 진행하겠습니다.\n",
    "\n",
    "1.mnist를 이미지로 변환하기\n",
    "\n",
    "2.이미지를 numpy로변환하기\n",
    "\n",
    "3.학습시키기\n",
    "\n",
    "4.mnist와 이미지 학습결과 비교하기\n",
    "\n",
    "\n",
    "\n",
    "##1.mnist를 이미지로 변환하기\n",
    "\n",
    "mnistsms 아래와 같이 손으로 쓰여진 이미지로 되어있습니다.\n",
    "<img src='https://i.imgur.com/HIQ6FNQ.png' >\n",
    "<img src='https://i.imgur.com/8HgRgz3.png'>\n",
    "\n",
    "먼저 mnist를 이미지로 변환 해주기 위해서 아래와 같이 import를 해줍니다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\venv\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그 다음 mnist 데이터셋을 로드해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.misc import imsave\n",
    "import numpy as n\n",
    "import matplotlib.pyplot as plt\n",
    "for i in range(np.shape(X_train)[0]): #60000\n",
    "    plt.imshow(X_train[i], cmap=plt.get_cmap('gray'))\n",
    "    plt.show()\n",
    "    path=\"D:/deeplearning1/mnist/\"+str(i)+\".png\"\n",
    "    imsave(path,X_test[i])\n",
    "\n",
    "for i in range(np.shape(X_test)[0]): #10000\n",
    "    plt.imshow(X_test[i], cmap=plt.get_cmap('gray'))\n",
    "    plt.show()\n",
    "    path=\"D:/deeplearning2/mnist/\"+str(i)+\".png\"\n",
    "    imsave(path,X_test[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이미지가 잘 나오는지 보기 위해 plt.imshow()를 해줬습니다. \n",
    "\n",
    "imsave를 통해 해당 path에 저장됩니다.\n",
    "\n",
    "결과를 보면 아래처럼 총 70000개가  저장이 되어있습니다.\n",
    "<img src='https://i.imgur.com/tf5bncz.png' width='400' height='200' >\n",
    "<img src='https://i.imgur.com/cf2QZaP.png' width='400' height='300'>\n",
    "##2.이미지를 numpy로 변환하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import PIL.Image as pilimg\n",
    "train=[]\n",
    "test=[]\n",
    "for i in range(60000):\n",
    "    train_generator = pilimg.open('D:deeplearning/mnist/'+str(i)+'.png')\n",
    "    train1 = np.array(train_generator) #이미지를 배열로 변환한 부분\n",
    "    print('train1:',train1)\n",
    "    train.append(train1)               #이미지를 배열에 넣은 것을 하나의 배열에 넣는 부분\n",
    "print(\"np.shape(train):\",np.shape(train))\n",
    "\n",
    "for i in range(10000):\n",
    "    test_generator = pilimg.open('D:deeplearning2/mnist/'+str(i)+'.png')\n",
    "    test1=np.array( test_generator)\n",
    "    print('test1:',test1)\n",
    "    test.append(test1)\n",
    "print(\"np.shape\",np.shape(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "밑에 사진은 하나의  이미지를 배열로 변환한 것입니다.\n",
    "<img src=\"https://i.imgur.com/ukIjVMM.png\">\n",
    "이미지를 배열로 변환한 것을 한 배열 안에 넣어주면, [[[]]]이런 모양의 3차원 배열이 됩니다.\n",
    "train의shape는 (60000, 28, 28)이 되고, test의 shape는 (10000.28,28)이 됩니다.\n",
    "\n",
    "##3.학습시키기\n",
    "numpy로 변환한것을 학습시키기 전에 mnist 예제에서 데이터셋 부분만 이미지를 numpy로 바꿔준 코드로 바꿔줍니다.\n",
    "mnist예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n",
      "dfdf (60000, 10)\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/12\n",
      "48384/60000 [=======================>......] - ETA: 53s - loss: 0.3071 - acc: 0.9047"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 12\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "print('dfdf',y_train.shape)\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "바뀐코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Trains a simple convnet on the MNIST dataset.\n",
    "Gets to 99.25% test accuracy after 12 epochs\n",
    "(there is still a lot of margin for parameter tuning).\n",
    "16 seconds per epoch on a GRID K520 GPU.\n",
    "'''\n",
    "\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import PIL.Image as pilimg\n",
    "\n",
    "train=[]\n",
    "test=[]\n",
    "\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 12\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\t#y_train과 y_test를 사용하기위해서\n",
    "\n",
    "for i in range(60000):\n",
    "    train_generator = pilimg.open('D:deeplearning/mnist/'+str(i)+'.png')\n",
    "    train1 = np.array(train_generator) #이미지를 배열로 변환한 부분\n",
    "    print('train1:',train1)\n",
    "    train.append(train1)\t\t\t  #이미지를 배열에 넣은 것을 하나의 배열에 넣는 부분\n",
    "print(\"np.shape(train):\",np.shape(train))\n",
    "\n",
    "for i in range(10000):\n",
    "    test_generator = pilimg.open('D:deeplearning2/mnist/'+str(i)+'.png')\n",
    "    test1=np.array( test_generator)\n",
    "    print('test1:',test1)\n",
    "    test.append(test1)\n",
    "print(\"np.shape\",np.shape(test))\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    train = np.reshape(train,(np.shape(train)[0], 1, img_rows, img_cols))\n",
    "    test = np.reshape(test,(np.shape(test)[0], 1, img_rows, img_cols))\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    train = np.reshape(train,(np.shape(trina)[0], img_rows, img_cols, 1))\n",
    "    test = np.reshape(test,(np.shape(test)[0], img_rows, img_cols, 1))\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "train = train.astype('float32')\n",
    "test = test.astype('float32')\n",
    "train /= 255\n",
    "test /= 255\n",
    "print('train shape:',np.shape(train))\n",
    "print(np.shape(train)[0], 'train samples')\n",
    "print(np.shape(test)[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "print('dfdf',y_train.shape)\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(test, y_test))\n",
    "score = model.evaluate(test, y_test, verbose=0)\n",
    "\n",
    "\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y값들은 사진에 나와있는 숫자들인데 총 7만개를 만들 수 없어서 mnist에 있는 것을 사용하였습니다.\n",
    "\n",
    "그리고 train.shape -> np.shape(train)\n",
    "\n",
    "\t   test.shape -> np.shape(test)\n",
    "\n",
    "\t   train.reshape()->np.reshape(train,()) \n",
    "\n",
    "\t   test.reshape()->np.reshape(test,()) 로 바꿔줍니다.\n",
    "\n",
    "바꿔주지 않는다면 AttributeError:'list' object has no attribute 'shape' 에러가 뜰것입니다.\n",
    "\n",
    "##4.mnist와 이미지 학습결과 비교하기\n",
    "\n",
    "mnist와 이미지의 학습결과를 비교하겠습니다.\n",
    "\n",
    "mnist의 학습결과는 0.9906이고, 이미지의 학습결과는 0.9886입니다.\n",
    "mnist의 결과가  0.002만큼 높지만, 거의 비슷하다고 할 수 있습니다\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
